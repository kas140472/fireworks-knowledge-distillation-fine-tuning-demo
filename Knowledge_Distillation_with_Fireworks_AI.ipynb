{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Knowledge Distillation with Fireworks AI\n",
        "\n",
        "Transfer knowledge from large teacher models to smaller, low-cost, more efficient student models while preserving performance.\n",
        "\n",
        "Knowledge distillation enables you to create compact models that maintain the reasoning capabilities of larger models. This tutorial demonstrates the complete workflow using GSM8K mathematical reasoning as our example task.\n",
        "\n",
        "| **Technique** | **Teacher Model** | **Student Model** | **Primary Goal** |\n",
        "|---------------|-------------------|-------------------|-------------------------|\n",
        "| **Supervised Fine-Tuning (SFT)** | DeepSeek-V3 (685B) | Qwen2.5-7B | Format Learning & Structure |\n",
        "| **Reinforcement Fine-Tuning (RFT)** | N/A (Self-improvement) | Fine tuned Qwen2.5-7B | Accuracy Optimization |"
      ],
      "metadata": {
        "id": "vBVqeb6N2ciI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Course Overview\n",
        "\n",
        "This tutorial demonstrates a systematic two-stage knowledge distillation pipeline:\n",
        "\n",
        "**Stage 1 - SFT (Format Learning)**:\n",
        "1. Generate training data with consistent output formatting\n",
        "2. Train student model to internalize structured response patterns\n",
        "3. Demonstrate format learning without explicit instructions\n",
        "\n",
        "**Stage 2 - RFT (Accuracy Improvement)**:\n",
        "4. Build reward system based on answer correctness\n",
        "5. Apply reinforcement learning to improve reasoning within learned format\n",
        "6. Show accuracy gains while maintaining consistent structure\n",
        "\n",
        "**Why This Two-Stage Approach Works**:\n",
        "- **SFT**: Excels at learning structural patterns and making them default behavior\n",
        "- **RFT**: Excels at optimizing content quality through reward-based learning  \n",
        "- **Together**: Create models that are both well-formatted AND more accurate"
      ],
      "metadata": {
        "id": "ANb1kdG_2evS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 1: Environment Setup\n",
        "\n",
        "**Requirements:**\n",
        "- Fireworks AI account with API access\n",
        "- Basic familiarity with fine-tuning concepts\n",
        "- Understanding of train/test splits for valid evaluation"
      ],
      "metadata": {
        "id": "JV8JETb82jo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install --upgrade fireworks-ai\n",
        "\n",
        "# Core imports for the entire course\n",
        "from fireworks import LLM, Dataset\n",
        "import fireworks\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import random\n",
        "from typing import List, Dict, Optional\n",
        "import os"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AEYofroCnJ5-",
        "outputId": "9b447f11-96df-477b-cd9b-25400fd999aa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fireworks-ai in /usr/local/lib/python3.11/dist-packages (0.19.16)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from fireworks-ai) (0.28.1)\n",
            "Requirement already satisfied: httpx-ws in /usr/local/lib/python3.11/dist-packages (from fireworks-ai) (0.7.2)\n",
            "Requirement already satisfied: httpx-sse in /usr/local/lib/python3.11/dist-packages (from fireworks-ai) (0.4.1)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from fireworks-ai) (2.11.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from fireworks-ai) (11.3.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (from fireworks-ai) (1.78.1)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from fireworks-ai) (4.14.1)\n",
            "Requirement already satisfied: mmh3>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from fireworks-ai) (5.2.0)\n",
            "Requirement already satisfied: betterproto-fw>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from betterproto-fw[compiler]>=2.0.3->fireworks-ai) (2.0.3)\n",
            "Requirement already satisfied: asyncstdlib-fw>=3.13.2 in /usr/local/lib/python3.11/dist-packages (from fireworks-ai) (3.13.2)\n",
            "Requirement already satisfied: grpcio>=1.71.0 in /usr/local/lib/python3.11/dist-packages (from fireworks-ai) (1.74.0)\n",
            "Requirement already satisfied: protobuf==5.29.3 in /usr/local/lib/python3.11/dist-packages (from fireworks-ai) (5.29.3)\n",
            "Requirement already satisfied: rich>=14.0.0 in /usr/local/lib/python3.11/dist-packages (from fireworks-ai) (14.1.0)\n",
            "Requirement already satisfied: reward-kit>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from fireworks-ai) (0.4.1)\n",
            "Requirement already satisfied: toml>=0.10.2 in /usr/local/lib/python3.11/dist-packages (from fireworks-ai) (0.10.2)\n",
            "Requirement already satisfied: aiohttp>=3.12.11 in /usr/local/lib/python3.11/dist-packages (from aiohttp[speedups]>=3.12.11->fireworks-ai) (3.12.15)\n",
            "Requirement already satisfied: attrs==23.2.0 in /usr/local/lib/python3.11/dist-packages (from fireworks-ai) (23.2.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.12.11->aiohttp[speedups]>=3.12.11->fireworks-ai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.12.11->aiohttp[speedups]>=3.12.11->fireworks-ai) (1.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.12.11->aiohttp[speedups]>=3.12.11->fireworks-ai) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.12.11->aiohttp[speedups]>=3.12.11->fireworks-ai) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.12.11->aiohttp[speedups]>=3.12.11->fireworks-ai) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.12.11->aiohttp[speedups]>=3.12.11->fireworks-ai) (1.20.1)\n",
            "Requirement already satisfied: aiodns>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp[speedups]>=3.12.11->fireworks-ai) (3.5.0)\n",
            "Requirement already satisfied: Brotli in /usr/local/lib/python3.11/dist-packages (from aiohttp[speedups]>=3.12.11->fireworks-ai) (1.1.0)\n",
            "Requirement already satisfied: grpclib<0.5.0,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from betterproto-fw>=2.0.3->betterproto-fw[compiler]>=2.0.3->fireworks-ai) (0.4.8)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from betterproto-fw>=2.0.3->betterproto-fw[compiler]>=2.0.3->fireworks-ai) (2.9.0.post0)\n",
            "Requirement already satisfied: ruff~=0.9.1 in /usr/local/lib/python3.11/dist-packages (from betterproto-fw[compiler]>=2.0.3->fireworks-ai) (0.9.10)\n",
            "Requirement already satisfied: jinja2>=3.0.3 in /usr/local/lib/python3.11/dist-packages (from betterproto-fw[compiler]>=2.0.3->fireworks-ai) (3.1.6)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.11/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (2.32.3)\n",
            "Requirement already satisfied: dataclasses-json>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (0.6.7)\n",
            "Requirement already satisfied: fastapi>=0.68.0 in /usr/local/lib/python3.11/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (0.116.1)\n",
            "Requirement already satisfied: uvicorn>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (0.35.0)\n",
            "Requirement already satisfied: python-dotenv>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (1.1.1)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (0.21.0)\n",
            "Requirement already satisfied: mcp>=1.9.2 in /usr/local/lib/python3.11/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (1.12.3)\n",
            "Requirement already satisfied: PyYAML>=5.0 in /usr/local/lib/python3.11/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (6.0.2)\n",
            "Requirement already satisfied: datasets==3.6.0 in /usr/local/lib/python3.11/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (3.6.0)\n",
            "Requirement already satisfied: fsspec==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (2025.3.0)\n",
            "Requirement already satisfied: hydra-core>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (1.3.2)\n",
            "Requirement already satisfied: omegaconf>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (2.3.0)\n",
            "Requirement already satisfied: gymnasium>=0.29.0 in /usr/local/lib/python3.11/dist-packages (from reward-kit>=0.3.1->fireworks-ai) (1.2.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai->fireworks-ai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai->fireworks-ai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai->fireworks-ai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai->fireworks-ai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai->fireworks-ai) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (2.3.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (0.70.16)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (0.34.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (25.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->fireworks-ai) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->fireworks-ai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->fireworks-ai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->fireworks-ai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->fireworks-ai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->fireworks-ai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->fireworks-ai) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=14.0.0->fireworks-ai) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=14.0.0->fireworks-ai) (2.19.2)\n",
            "Requirement already satisfied: wsproto in /usr/local/lib/python3.11/dist-packages (from httpx-ws->fireworks-ai) (1.2.0)\n",
            "Requirement already satisfied: pycares>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from aiodns>=3.3.0->aiohttp[speedups]>=3.12.11->fireworks-ai) (4.9.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json>=0.5.7->reward-kit>=0.3.1->fireworks-ai) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json>=0.5.7->reward-kit>=0.3.1->fireworks-ai) (0.9.0)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.68.0->reward-kit>=0.3.1->fireworks-ai) (0.47.2)\n",
            "Requirement already satisfied: h2<5,>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from grpclib<0.5.0,>=0.4.1->betterproto-fw>=2.0.3->betterproto-fw[compiler]>=2.0.3->fireworks-ai) (4.2.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.29.0->reward-kit>=0.3.1->fireworks-ai) (3.1.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.29.0->reward-kit>=0.3.1->fireworks-ai) (0.0.4)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from hydra-core>=1.3.2->reward-kit>=0.3.1->fireworks-ai) (4.9.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=3.0.3->betterproto-fw[compiler]>=2.0.3->fireworks-ai) (3.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=14.0.0->fireworks-ai) (0.1.2)\n",
            "Requirement already satisfied: jsonschema>=4.20.0 in /usr/local/lib/python3.11/dist-packages (from mcp>=1.9.2->reward-kit>=0.3.1->fireworks-ai) (4.25.0)\n",
            "Requirement already satisfied: pydantic-settings>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from mcp>=1.9.2->reward-kit>=0.3.1->fireworks-ai) (2.10.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.11/dist-packages (from mcp>=1.9.2->reward-kit>=0.3.1->fireworks-ai) (0.0.20)\n",
            "Requirement already satisfied: sse-starlette>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from mcp>=1.9.2->reward-kit>=0.3.1->fireworks-ai) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.8.0->betterproto-fw>=2.0.3->betterproto-fw[compiler]>=2.0.3->fireworks-ai) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->reward-kit>=0.3.1->fireworks-ai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->reward-kit>=0.3.1->fireworks-ai) (2.5.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.15.0->reward-kit>=0.3.1->fireworks-ai) (8.2.1)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3.1.0->grpclib<0.5.0,>=0.4.1->betterproto-fw>=2.0.3->betterproto-fw[compiler]>=2.0.3->fireworks-ai) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3.1.0->grpclib<0.5.0,>=0.4.1->betterproto-fw>=2.0.3->betterproto-fw[compiler]>=2.0.3->fireworks-ai) (4.1.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (1.1.5)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.20.0->mcp>=1.9.2->reward-kit>=0.3.1->fireworks-ai) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.20.0->mcp>=1.9.2->reward-kit>=0.3.1->fireworks-ai) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.20.0->mcp>=1.9.2->reward-kit>=0.3.1->fireworks-ai) (0.26.0)\n",
            "Requirement already satisfied: cffi>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from pycares>=4.9.0->aiodns>=3.3.0->aiohttp[speedups]>=3.12.11->fireworks-ai) (1.17.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json>=0.5.7->reward-kit>=0.3.1->fireworks-ai) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==3.6.0->reward-kit>=0.3.1->fireworks-ai) (2025.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.5.0->pycares>=4.9.0->aiodns>=3.3.0->aiohttp[speedups]>=3.12.11->fireworks-ai) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### API Configuration"
      ],
      "metadata": {
        "id": "DHkyhabclSfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your Fireworks API key (get one at https://app.fireworks.ai/settings/users/api-keys)\n",
        "# fireworks.client.api_key = 'your-api-key-here'\n",
        "\n",
        "# Test SDK connection\n",
        "llm = LLM(model=\"llama4-maverick-instruct-basic\", deployment_type=\"serverless\")\n",
        "\n",
        "response = llm.chat.completions.create(\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Hello! Can you help me learn about AI?\"}]\n",
        ")\n",
        "\n",
        "print(\"SDK Connection Test:\")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "uYzmDp9TfiIF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aac07744-7f37-4828-8acb-2cb9d7002e1b",
        "collapsed": true
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SDK Connection Test:\n",
            "I'd be delighted to help you learn about AI.\n",
            "\n",
            "Artificial Intelligence (AI) is a vast and exciting field that has been rapidly evolving over the past few decades. To get started, let's break down the basics:\n",
            "\n",
            "**What is AI?**\n",
            "AI refers to the development of computer systems that can perform tasks that typically require human intelligence, such as:\n",
            "\n",
            "1. Learning\n",
            "2. Problem-solving\n",
            "3. Reasoning\n",
            "4. Perception (e.g., image and speech recognition)\n",
            "5. Decision-making\n",
            "\n",
            "**Types of AI:**\n",
            "There are several types of AI, including:\n",
            "\n",
            "1. **Narrow or Weak AI**: Designed to perform a specific task, like image recognition, language translation, or playing chess.\n",
            "2. **General or Strong AI**: A hypothetical AI that can perform any intellectual task that a human can.\n",
            "3. **Superintelligence**: A hypothetical AI that is significantly more intelligent than the best human minds.\n",
            "\n",
            "**Key concepts:**\n",
            "\n",
            "1. **Machine Learning (ML)**: A subset of AI that involves training algorithms to learn from data and improve their performance over time.\n",
            "2. **Deep Learning (DL)**: A type of ML that uses neural networks to analyze complex data, like images and speech.\n",
            "3. **Natural Language Processing (NLP)**: A field that focuses on enabling computers to understand, interpret, and generate human language.\n",
            "\n",
            "**Applications of AI:**\n",
            "AI has many practical applications across various industries, including:\n",
            "\n",
            "1. **Virtual assistants** (e.g., Siri, Alexa)\n",
            "2. **Image and speech recognition** (e.g., self-driving cars, facial recognition)\n",
            "3. **Healthcare** (e.g., medical diagnosis, personalized medicine)\n",
            "4. **Customer service** (e.g., chatbots, customer support)\n",
            "5. **Finance** (e.g., predictive analytics, risk management)\n",
            "\n",
            "**Getting started:**\n",
            "To learn more about AI, you can:\n",
            "\n",
            "1. Take online courses or tutorials (e.g., Coursera, edX, Udemy)\n",
            "2. Read books or articles on AI and related topics\n",
            "3. Explore AI-powered tools and applications\n",
            "4. Join online communities or forums (e.g., Reddit's r/MachineLearning and r/AI)\n",
            "\n",
            "What specific aspect of AI are you interested in learning more about?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What's Happening Here:**\n",
        "\n",
        "- Fireworks SDK: Simplified interface for model deployment and fine-tuning\n",
        "- Serverless Models: Pre-deployed models you can use immediately\n",
        "- API Key: Authenticates your requests and tracks usage"
      ],
      "metadata": {
        "id": "qdlKHNISnY4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 2: Dataset Preparation and Analysis"
      ],
      "metadata": {
        "id": "462bvgJd21XX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why GSM8K?**\n",
        "- **Standard Benchmark**: Widely used for evaluating mathematical reasoning\n",
        "- **Clear Evaluation**: Numerical answers are easy to check for correctness\n",
        "- **Appropriate Difficulty**: Challenging enough to demonstrate knowledge transfer"
      ],
      "metadata": {
        "id": "HFu__c2yoLo5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why We Need Proper Train/Test Splits**\n",
        "\n",
        "**Critical for Valid Evaluation**: Using the same data for training and testing leads to inflated results that don't reflect real-world performance. GSM8K provides standard splits that enable fair comparison with other research."
      ],
      "metadata": {
        "id": "cAaaNpvs2_Ln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load GSM8K Dataset"
      ],
      "metadata": {
        "id": "Li4fIUWz3Bxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load both splits\n",
        "splits = {\n",
        "    'train': 'main/train-00000-of-00001.parquet',\n",
        "    'test': 'main/test-00000-of-00001.parquet'\n",
        "}\n",
        "\n",
        "# Load train set\n",
        "df_train = pd.read_parquet(\"hf://datasets/openai/gsm8k/\" + splits[\"train\"])\n",
        "\n",
        "# Load test set\n",
        "df_test = pd.read_parquet(\"hf://datasets/openai/gsm8k/\" + splits[\"test\"])"
      ],
      "metadata": {
        "id": "UEP4Bp_e3Dty",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b38f2b6c-3b3a-4898-991a-dd7f0fd8a3f7",
        "collapsed": true
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Dataset Statistics:\n",
        "  • Train size: 7473\n",
        "  • Test size: 1319\n",
        "  • Total: 8792\n",
        "```\n"
      ],
      "metadata": {
        "id": "maDYDQCTEMYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example GSM8K Problem:**"
      ],
      "metadata": {
        "id": "DfVbV8P23ou6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "{\n",
        "    'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
        "    'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72',\n",
        "}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ikseyW6Y4nx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why This Format Matters**: The `#### 18` format provides the ground truth answer we need for automated evaluation. We'll extract this pattern to check model correctness."
      ],
      "metadata": {
        "id": "HVGd7T129MMM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Process Dataset for Training and Evaluation**"
      ],
      "metadata": {
        "id": "NjVkErZ-2JXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gsm8k_train_problems = []\n",
        "for idx, row in df_train.iterrows():\n",
        "    answer_match = re.search(r'#### (\\d+)', row['answer'])\n",
        "    ground_truth = answer_match.group(1) if answer_match else None\n",
        "\n",
        "    if ground_truth:\n",
        "        gsm8k_train_problems.append({\n",
        "            \"question\": row['question'],\n",
        "            \"ground_truth\": ground_truth,\n",
        "            \"full_solution\": row['answer']\n",
        "        })\n",
        "\n",
        "gsm8k_test_problems = []\n",
        "for idx, row in df_test.iterrows():\n",
        "    answer_match = re.search(r'#### (\\d+)', row['answer'])\n",
        "    ground_truth = answer_match.group(1) if answer_match else None\n",
        "\n",
        "    if ground_truth:\n",
        "        gsm8k_test_problems.append({\n",
        "            \"question\": row['question'],\n",
        "            \"ground_truth\": ground_truth,\n",
        "            \"full_solution\": row['answer']\n",
        "        })"
      ],
      "metadata": {
        "id": "rvtIBAPl9euU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 3: Model Setup"
      ],
      "metadata": {
        "id": "j_Kq61QbqOko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deploy Your Student Model"
      ],
      "metadata": {
        "id": "Klza-9nAfD8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Selection**: We're using [Qwen2.5-7B](https://fireworks.ai/models/fireworks/qwen2p5-7b) as our student model because:\n",
        "- **Right Size**: Large enough to learn complex patterns, small enough to be efficient\n",
        "- **Strong Base**: Pre-trained on diverse data including mathematical content\n",
        "- **Cost-Effective**: Significantly cheaper to run than larger models"
      ],
      "metadata": {
        "id": "IJQtEMIEgvN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Deploy the base model for training and inference\n",
        "base_llm = LLM(\n",
        "    model=\"qwen2p5-7b\",\n",
        "    id=\"kd-base-model\",  # Unique identifier\n",
        "    deployment_type=\"on-demand\",  # Scales automatically\n",
        "    min_replica_count=0,\n",
        "    max_replica_count=1\n",
        ")\n",
        "\n",
        "# Apply the deployment configuration\n",
        "base_llm.apply()"
      ],
      "metadata": {
        "id": "cKjJeFVffKHD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f34ad01-851e-47be-92cf-b09bb89c3d2e",
        "collapsed": true
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLM(model=accounts/fireworks/models/qwen2p5-7b, deployment_url=https://app.fireworks.ai/dashboard/deployments/kd-base-model, deployment_type=on-demand, min_replica_count=0, max_replica_count=1)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Baseline Model Behavior"
      ],
      "metadata": {
        "id": "fjZZzMC4sR4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test our baseline model on a sample problem\n",
        "sample_question = \"Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much does she make every day at the farmers' market?\"\n",
        "\n",
        "baseline_response = base_llm.chat.completions.create(\n",
        "    messages=[{\"role\": \"user\", \"content\": sample_question}],\n",
        "    max_tokens = 10000\n",
        ")\n",
        "\n",
        "baseline_response.choices[0].message.content"
      ],
      "metadata": {
        "id": "fzeDTIUFsTOW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "collapsed": true,
        "outputId": "c1759ab2-c17a-4da7-c32e-df167b5aa389"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Janet has 16 eggs per day. She eats 3 into breakfast, leaving her with 16-3 = 13 eggs. Out of these, she uses 4 for her muffin recipes, which results in 13-4 = 9 eggs left. Selling each of these leftover eggs at $2, she makes 9*2 = $18 per day at the market.\\n\\nprint(9*2)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected Baseline Behavior**: Unstructured, verbose responses without consistent formatting patterns."
      ],
      "metadata": {
        "id": "pJnocJGYqQ2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Actual Baseline Model Outputs:**"
      ],
      "metadata": {
        "id": "gbAtP0Mc9n10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output 1:\n",
        "\n",
        "```\n",
        "Janet has 16 eggs per day. She eats 3 into breakfast, leaving her with 16-3 = 13 eggs. Out of these, she uses 4 for her muffin recipes, which results in 13-4 = 9 eggs left. Selling each of these leftover eggs at $2, she makes 9*2 = $18 per day at the market.\n",
        "\n",
        "print(9*2)\n",
        "```\n",
        "\n",
        "\n",
        "Output 2:\n",
        "```\n",
        "Janet starts with 16 ducks eggs. Each day, she eats 3 for breakfast and uses 4 for her muffins, which totals 7 eggs.\n",
        "\n",
        "The remainder she sells. So, the remaining eggs are 16 - 7. She sells these at $2 per egg.\n",
        "\n",
        "We can calculate her daily earnings from selling eggs with this simple math. I will write a python code snippet to perform this calculation.\n",
        "```python\n",
        "# Number of eggs laid by ducks daily\n",
        "laying_daily = 16\n",
        "\n",
        "# Number of eggs used by Janet and her friends\n",
        "eggs_for_use = 3 + 4\n",
        "\n",
        "# Number of eggs remaining to sell\n",
        "remaining_eggs = laying_daily - eggs_for_use\n",
        "\n",
        "#_price per fresh duck egg\n",
        "price_per_egg = 2\n",
        "\n",
        "# Daily earnings by selling the remaining eggs\n",
        "daily_cool = remaining_eggs * price_per_egg\n",
        "print(daily_cool)\n",
        "\n",
        "output\n",
        "20\n",
        "\n",
        "Janet sells the remainder eggs at the farmers' market, making \\$20 per day.\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sMo_pSxJ9TiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 4: Stage 1 - Supervised Fine-Tuning (SFT)"
      ],
      "metadata": {
        "id": "J9SBgWeRsZeV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Formatted Training Data with Teacher Model\n",
        "\n",
        "#### Why Use a Teacher Model\n",
        "\n",
        "**The Knowledge Transfer Principle**\n",
        "\n",
        "Rather than learning math reasoning from scratch, we'll have a powerful model (DeepSeek-V3) solve problems step-by-step, then train our small model to mimic those high-quality solutions.\n",
        "\n",
        "**Why DeepSeek-V3**:\n",
        "\n",
        "- **Strong mathematical reasoning** (>90% on GSM8K)\n",
        "- **Clear step-by-step explanations** that provide good learning examples\n",
        "- **Consistent output format** when given proper instructions\n",
        "- **Cost-effective** for generating training data (no deployment required)\n",
        "- **Available as serverless model on Fireworks AI platform**\n",
        "\n",
        "**Two-Stage Data Strategy**: We'll generate one high-quality dataset from our teacher model and adapt it for both training stages:\n",
        "\n",
        "- **Stage 1 (SFT)**: Use teacher responses as training targets to learn format patterns\n",
        "- **Stage 2 (RFT)**: Use the same problems with ground truth labels for reward-based learning"
      ],
      "metadata": {
        "id": "R7rNfRCy009T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining Our Target Format\n",
        "\n",
        "**Why Structured Output?**\n",
        "- **Consistency**: Every response follows the same pattern\n",
        "- **Parseability**: Easy to extract answers programmatically\n",
        "- **Debugging**: Clear separation of reasoning and results\n",
        "- **Production Ready**: Reliable format for downstream applications\n",
        "- **Unique**: Different from typical model outputs\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "TARGET_FORMAT_EXAMPLE = \"\"\"\n",
        "[WORK]\n",
        "1. Janet's ducks lay 16 eggs per day\n",
        "2. She eats 3 eggs for breakfast  \n",
        "3. She uses 4 eggs for muffins\n",
        "4. Remaining eggs: 16 - 3 - 4 = 9 eggs\n",
        "5. Revenue: 9 eggs × $2/egg = $18\n",
        "[/WORK]\n",
        "\n",
        "[RESULT]\n",
        "18\n",
        "[/RESULT]\n",
        "\"\"\"\n",
        "```"
      ],
      "metadata": {
        "id": "MSJ5h1u7rUC4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Teaching the Teacher Model Our Format\n",
        "\n",
        "**Strategy**: We'll use a system prompt to teach our teacher model (DeepSeek-V3) to use our desired format, then capture those formatted responses as training data."
      ],
      "metadata": {
        "id": "Ub4vzdCvLB34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# System prompt that teaches the format\n",
        "SYSTEM_PROMPT = \"\"\"You are a math tutor. When solving problems, always structure your response in this exact format:\n",
        "\n",
        "[WORK]\n",
        "Show your step-by-step reasoning here. Work through the problem systematically, showing calculations and logic clearly.\n",
        "[/WORK]\n",
        "\n",
        "[RESULT]\n",
        "Put only the final numerical answer here (no units, no extra text)\n",
        "[/RESULT]\n",
        "\n",
        "Follow this format exactly for every math problem.\"\"\""
      ],
      "metadata": {
        "id": "Oo32o68Gr4_G"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the teacher model with our format instructions\n",
        "sample_question = \"Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much does she make every day at the farmers' market?\"\n",
        "\n",
        "messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": sample_question}]\n",
        "\n",
        "teacher_llm = LLM(model=\"deepseek-v3\", deployment_type=\"serverless\")\n",
        "\n",
        "teacher_response = teacher_llm.chat.completions.create(\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "teacher_response.choices[0].message.content"
      ],
      "metadata": {
        "id": "oIEEEaZQLN--",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "collapsed": true,
        "outputId": "641e54fa-cf95-4429-f849-f0d8dd25be7d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[WORK]\\n1. Janet's ducks lay 16 eggs per day.\\n2. She eats 3 eggs for breakfast daily.\\n3. She uses 4 eggs for baking muffins daily.\\n4. Total eggs used or consumed: \\\\(3 + 4 = 7\\\\)\\n5. Eggs remaining for sale: \\\\(16 - 7 = 9\\\\)\\n6. Price per egg: \\\\$2\\n7. Daily earnings at the farmers' market: \\\\(9 \\\\times 2 = 18\\\\) \\n[/WORK]\\n\\n[RESULT]\\n18\\n[/RESULT]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Actual teacher model response:**\n",
        "\n",
        "```\n",
        "[WORK]\n",
        "1. Janet's ducks lay 16 eggs per day.\n",
        "2. She eats 3 eggs for breakfast daily.\n",
        "3. She uses 4 eggs for baking muffins daily.\n",
        "4. Total eggs used or consumed: \\(3 + 4 = 7\\)\n",
        "5. Eggs remaining for sale: \\(16 - 7 = 9\\)\n",
        "6. Price per egg: \\$2\n",
        "7. Daily earnings at the farmers' market: \\(9 \\times 2 = 18\\)\n",
        "[/WORK]\n",
        "\n",
        "[RESULT]\n",
        "18\n",
        "[/RESULT]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "4S39DdBiLy1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating High-Quality Training Data\n",
        "\n",
        "**The Process**:\n",
        "1. Take problems from GSM8K training set\n",
        "2. Have teacher model solve them using our format\n",
        "3. Verify teacher got the right answer\n",
        "4. Create training examples from successful solutions"
      ],
      "metadata": {
        "id": "W_s7gjcNMaeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_answer_from_result_tags(response: str) -> str:\n",
        "    \"\"\"Extract answer from [RESULT] tags\"\"\"\n",
        "    result_match = re.search(r'\\[RESULT\\](.*?)\\[/RESULT\\]', response, re.DOTALL)\n",
        "    if result_match:\n",
        "        return result_match.group(1).strip()\n",
        "    return None\n",
        "\n",
        "def generate_sft_training_data(train_problems_sample):\n",
        "    \"\"\"Generate training data using teacher model with format instructions\"\"\"\n",
        "\n",
        "    sft_dataset = []\n",
        "    successful_examples = 0\n",
        "\n",
        "    for i, problem in enumerate(train_problems_sample):\n",
        "\n",
        "        # Get teacher response with format instructions\n",
        "        messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": problem[\"question\"]}]\n",
        "\n",
        "        teacher_llm = LLM(model=\"deepseek-v3\", deployment_type=\"serverless\")\n",
        "\n",
        "        teacher_response_obj = teacher_llm.chat.completions.create(\n",
        "            messages=messages\n",
        "        )\n",
        "\n",
        "        teacher_response = teacher_response_obj.choices[0].message.content\n",
        "\n",
        "        # Check if teacher got the right answer\n",
        "        teacher_answer = extract_answer_from_result_tags(teacher_response)\n",
        "\n",
        "        # Only include if teacher got the answer right AND used proper format\n",
        "        if teacher_answer == problem[\"ground_truth\"] and \"[WORK]\" in teacher_response and \"[RESULT]\" in teacher_response:\n",
        "            # Don't include system prompt in training data so model learns\n",
        "            # that the format should be followed even when not in system prompt\n",
        "            training_example = {\n",
        "                \"messages\": [\n",
        "                    {\"role\": \"user\", \"content\": problem[\"question\"]},\n",
        "                    {\"role\": \"assistant\", \"content\": teacher_response}\n",
        "                ]\n",
        "            }\n",
        "            sft_dataset.append(training_example)\n",
        "            successful_examples += 1\n",
        "\n",
        "    return sft_dataset, successful_examples\n",
        "\n",
        "random.seed(42)\n",
        "sampled_problems = random.sample(gsm8k_train_problems, 10)\n",
        "\n",
        "# Generate SFT training data\n",
        "sft_training_data, successful_count = generate_sft_training_data(\n",
        "    sampled_problems\n",
        ")"
      ],
      "metadata": {
        "id": "fDLvruGIsd6s",
        "collapsed": true
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Actual result:**\n",
        "\n",
        "```\n",
        "Generated 951 high-quality training examples\n",
        "Teacher success rate: 951/1000 examples\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "grvBYwFSHmne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Uploading Training Data to Fireworks"
      ],
      "metadata": {
        "id": "KfRxLB5ONaPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to file first\n",
        "dataset_filename = \"kd_sft_dataset.jsonl\"\n",
        "with open(dataset_filename, 'w') as f:\n",
        "    for example in sft_training_data:\n",
        "        f.write(json.dumps(example) + '\\n')\n",
        "\n",
        "# Upload to Fireworks\n",
        "dataset = Dataset.from_file(dataset_filename)\n",
        "dataset.sync()"
      ],
      "metadata": {
        "id": "NQFHutaDNfXA"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SFT Training Configuration"
      ],
      "metadata": {
        "id": "zkT1zJwCspjW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Supervised Fine-Tuning Job**:\n",
        "  - **Model**: `Qwen2.5 7B`\n",
        "  - **Dataset**: dataset (Your uploaded dataset)  \n",
        "  - **Epochs**: 5-8 (format learning needs repetition)\n",
        "  - **Learning Rate**: 0.0005 (higher rate to override existing patterns)\n",
        "\n",
        "**Critical Parameters for Format Learning**:\n",
        "- **Higher Learning Rate**: Needed to override existing response patterns\n",
        "- **More Epochs**: Format internalization requires repetition\n",
        "- **Larger Model**: 3B+ has capacity to learn complex structural patterns\n",
        "- **No System Prompts in Training**: Teaches default behavior, not instruction-following"
      ],
      "metadata": {
        "id": "q0koemtVsqun"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running the SFT Training Job"
      ],
      "metadata": {
        "id": "rbNshzEIfcmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create fine-tuning job\n",
        "job = base_llm.create_supervised_fine_tuning_job(\n",
        "    display_name=\"kd-sft-job\",\n",
        "    dataset_or_id=dataset,\n",
        "    epochs=1,\n",
        "    learning_rate=1e-5\n",
        ")\n",
        "\n",
        "job.wait_for_completion()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDWp2p8XNOuL",
        "outputId": "807e8d82-04f3-49a8-8cbb-b3e4f9187b63",
        "collapsed": true
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<fireworks.supervised_fine_tuning_job.supervised_fine_tuning_job.SupervisedFineTuningJob at 0x7ba75b96c6d0>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deploying the Fine-Tuned Model"
      ],
      "metadata": {
        "id": "iNuUwzUifgPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sft_llm = LLM(\n",
        "    model=job.output_model,\n",
        "    deployment_type=\"on-demand\",\n",
        "    id=\"kd-sft-model\",\n",
        "    min_replica_count=0,\n",
        "    max_replica_count=1\n",
        ")\n",
        "sft_llm.apply()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RxXIsm_U2VF",
        "outputId": "7d50092e-b61b-4835-8ddd-7771f58b7ad4",
        "collapsed": true
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLM(model=accounts/pyroworks/models/ft-pamezugl-a1ycw, deployment_url=https://app.fireworks.ai/dashboard/deployments/kd-sft-model, deployment_type=on-demand, min_replica_count=0, max_replica_count=1)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 5: Evaluating SFT Results"
      ],
      "metadata": {
        "id": "QpzvLmOfstkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Format Learning Success\n",
        "\n",
        "**The Critical Test**: Can our fine-tuned model use the target format WITHOUT being explicitly told to do so?"
      ],
      "metadata": {
        "id": "9cxPgcSjtMca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_question = \"Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much does she make every day at the farmers' market?\"\n",
        "\n",
        "sft_llm_response = sft_llm.chat.completions.create(\n",
        "    messages=[{\"role\": \"user\", \"content\": sample_question}]\n",
        ")\n",
        "\n",
        "sft_llm_response.choices[0].message.content"
      ],
      "metadata": {
        "id": "6CopA526swWt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "c434a901-a974-4001-cd74-44141c18154c",
        "collapsed": true
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Janet's ducks produce 16 eggs per day. She consumes 3 eggs in the morning for breakfast, and uses 4 eggs daily to make muffins for her friends. That means she uses/gives away 3 + 4 = 7 eggs. \\n\\nSo then, the number of eggs she sells every day is 16 (total laid) - 7 (used/consumed) = 9 eggs.\\n\\nSince she sells each egg for $2, her daily earnings from selling eggs at the market are 9 (eggs sold) * $2 (price per egg) = $18.\\n\\nTherefore, Janet makes $18 every day at the farmers' market from selling duck eggs.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Actual output:**\n",
        "\n",
        "\n",
        "```\n",
        "<think>\n",
        "Okay, let's see. Janet's ducks lay 16 eggs each day. She eats 3 eggs every morning for breakfast. Then she bakes muffins and uses 4 eggs for that. The rest she sells at the market for $2 each. So, I need to calculate how much she makes daily from the sales.\n",
        "\n",
        "First, I'll find out how many eggs she has after eating and baking. So, total eggs laid per day is 16. She eats 3, so 16 - 3 = 13 eggs left. Then she uses 4 eggs for muffins, so 13 - 4 = 9 eggs remaining.\n",
        "\n",
        "Now, she sells these 9 eggs at $2 each. So, 9 eggs * $2 = $18. That should be her daily earnings from the market.\n",
        "</think>\n",
        "\n",
        "[WORK]\n",
        "1. Total eggs laid per day: 16\n",
        "2. Eggs eaten for breakfast: 3\n",
        "3. Eggs used for muffins: 4\n",
        "4. Eggs remaining after eating and baking: 16 - 3 - 4 = 9\n",
        "5. Price per egg: $2\n",
        "6. Total earnings from farmers' market: 9 * 2 = 18\n",
        "[/WORK]\n",
        "\n",
        "[RESULT]\n",
        "18\n",
        "[/RESULT]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "sliFeeG1yaMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SUCCESS! SFT taught the model to automatically use the target format!**\n",
        "\n",
        "This demonstrates how SFT can make structural patterns the model's default behavior.\n",
        "\n",
        "If your format learning is incomplete, consider:\n",
        "- More training examples (aim for 1000+)\n",
        "- Higher learning rate (try 5e-5)  \n",
        "- More epochs (try 5-8)\n",
        "- Verify training data format consistency\n",
        "\n",
        "Now that we have consistent, structured responses, we can focus purely on improving the *quality* of the content within that structure. This is where Stage 2 (RFT) shines - optimizing for correctness while maintaining our learned formatting."
      ],
      "metadata": {
        "id": "NcmddFKO4Fu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding SFT's Strengths and Limitations\n",
        "\n",
        "Strengths demonstrated\n",
        "\n",
        "- Consistent output formatting\n",
        "- No system prompts needed\n",
        "- Internalized behavior patterns\n",
        "\n",
        "Limitations to address\n",
        "\n",
        "- Accuracy may not improve dramatically\n",
        "- Only mimics teacher, doesn't generalize\n",
        "- No feedback loop for corrections"
      ],
      "metadata": {
        "id": "cz4KxRi7tnKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 6: Stage 2 - Reinforcement Fine-Tuning (RFT)"
      ],
      "metadata": {
        "id": "gBpTs1yes_Hc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our model consistently uses the `[WORK]` and `[RESULT]` format **automatically** (without being told), we can apply RFT to improve the accuracy of answers within that structure.\n",
        "\n",
        "### Why Add Reinforcement Learning\n",
        "\n",
        "**Beyond Imitation**: While SFT teaches the model to mimic the teacher's style, RFT optimizes for **correctness**. The model learns to:\n",
        "- Prefer reasoning paths that lead to correct answers\n",
        "- Self-correct when making mistakes  \n",
        "- Develop confidence in its mathematical reasoning\n",
        "\n",
        "**How RFT Works**: Instead of just copying teacher responses, RFT gives the model a reward (+1) for correct answers and penalty (0) for wrong answers, encouraging the model to find its own path to the right solution.\n",
        "\n",
        "**RFT Advantages with SFT Foundation**:\n",
        "- Easy reward calculation from `[RESULT]` tags  \n",
        "- Maintains learned formatting while optimizing correctness\n",
        "- Builds on internalized structure to focus purely on accuracy\n",
        "- Shows the power of the two-stage approach"
      ],
      "metadata": {
        "id": "1dEcDXqhtKRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the RFT Dataset\n",
        "\n",
        "**Strategy**: Reuse the same problems our teacher model solved correctly during SFT generation, but format them for reinforcement learning."
      ],
      "metadata": {
        "id": "ODd2pzmPtOiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_rft_dataset_from_sft(sft_training_data, max_samples=1000):\n",
        "    \"\"\"\n",
        "    Create RFT dataset by extracting problems from existing SFT dataset\n",
        "    \"\"\"\n",
        "\n",
        "    rft_data = []\n",
        "    problems_processed = 0\n",
        "\n",
        "    for sft_example in sft_training_data:\n",
        "        if problems_processed >= max_samples:\n",
        "            break\n",
        "\n",
        "        user_question = None\n",
        "        teacher_response = None\n",
        "\n",
        "        # Extract user question and teacher response from messages\n",
        "        for message in sft_example[\"messages\"]:\n",
        "            if message[\"role\"] == \"user\":\n",
        "                user_question = message[\"content\"]\n",
        "            elif message[\"role\"] == \"assistant\":\n",
        "                teacher_response = message[\"content\"]\n",
        "\n",
        "        if user_question and teacher_response:\n",
        "            # Extract ground truth from teacher's [RESULT] tags\n",
        "            ground_truth = extract_answer_from_result_tags(teacher_response)\n",
        "\n",
        "            if ground_truth:\n",
        "                rft_example = {\n",
        "                    \"messages\": [\n",
        "                        {\"role\": \"user\", \"content\": user_question}\n",
        "                    ],\n",
        "                    \"ground_truth\": ground_truth\n",
        "                }\n",
        "                rft_data.append(rft_example)\n",
        "                problems_processed += 1\n",
        "    return rft_data\n",
        "\n",
        "# Create RFT dataset from our existing SFT dataset\n",
        "rft_training_data = create_rft_dataset_from_sft(sft_training_data, max_samples=1000)\n",
        "\n",
        "# Save to file\n",
        "dataset_filename = \"kd_rft_dataset.jsonl\"\n",
        "with open(dataset_filename, 'w') as f:\n",
        "    for example in rft_training_data:\n",
        "        f.write(json.dumps(example) + '\\n')\n",
        "\n",
        "# Upload dataset to Fireworks\n",
        "dataset = Dataset.from_file(\"kd_rft_dataset.jsonl\")\n",
        "dataset.sync()"
      ],
      "metadata": {
        "id": "gCKDJ6O3tQPy"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is what an RFT training data point looks like:\n",
        "\n",
        "\n",
        "```\n",
        "{\"messages\": [{\"role\": \"user\", \"content\": \"There are enough provisions in a castle to feed 300 people for 90 days. After 30 days, 100 people leave the castle. How many more days are left until all the food runs out?\"}], \"ground_truth\": \"90\"}\n",
        "```"
      ],
      "metadata": {
        "id": "5AT0qhq2X84J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding Reward Kit and Evaluators\n",
        "\n",
        "**What is Reward Kit?**\n",
        "Reward Kit is Fireworks AI's framework for creating custom evaluation functions for reinforcement learning. Think of it as the \"grading system\" that tells the model whether its answers are right or wrong."
      ],
      "metadata": {
        "id": "hkHqoJ0HtXI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a comprehensive evaluator for math problems\n",
        "\n",
        "rft_evaluator_code = '''\n",
        "import re\n",
        "from reward_kit import reward_function\n",
        "from reward_kit.models import EvaluateResult\n",
        "\n",
        "@reward_function\n",
        "def evaluate(messages: list[dict], **kwargs) -> EvaluateResult:\n",
        "    \"\"\"\n",
        "    RFT Evaluator: Compare model answer with ground truth\n",
        "    Optimized for [WORK]/[RESULT] format from SFT stage\n",
        "    \"\"\"\n",
        "\n",
        "    # Get ground truth from dataset\n",
        "    ground_truth_answer = kwargs.get('ground_truth')\n",
        "    if not ground_truth_answer:\n",
        "        return EvaluateResult(score=0.0, reason=\"No ground truth found in dataset\")\n",
        "\n",
        "    # Get the model's generated response (last message)\n",
        "    model_response = messages[-1][\"content\"]\n",
        "\n",
        "    # Extract model's answer using multiple methods\n",
        "    model_answer = extract_model_answer(model_response)\n",
        "\n",
        "    if not model_answer:\n",
        "        return EvaluateResult(score=0.0, reason=\"No answer extracted from model response\")\n",
        "\n",
        "    # Clean and compare answers\n",
        "    ground_truth_clean = clean_answer(ground_truth_answer)\n",
        "    model_answer_clean = clean_answer(model_answer)\n",
        "\n",
        "    if model_answer_clean == ground_truth_clean:\n",
        "        return EvaluateResult(score=1.0, reason=f\"Correct: {model_answer_clean}\")\n",
        "    else:\n",
        "        return EvaluateResult(score=0.0, reason=f\"Wrong: {model_answer_clean} vs {ground_truth_clean}\")\n",
        "\n",
        "def extract_model_answer(text: str) -> str:\n",
        "    \"\"\"Extract answer from model response, prioritizing our learned format\"\"\"\n",
        "\n",
        "    # Method 1: [RESULT] tags (primary method for our SFT model)\n",
        "    result_match = re.search(r'\\\\[RESULT\\\\](.*?)\\\\[/RESULT\\\\]', text, re.DOTALL)\n",
        "    if result_match:\n",
        "        return result_match.group(1).strip()\n",
        "\n",
        "    # Method 2: \\\\boxed{} format (fallback)\n",
        "    boxed_match = re.search(r'\\\\\\\\boxed\\\\{([^}]+)\\\\}', text)\n",
        "    if boxed_match:\n",
        "        return boxed_match.group(1).strip()\n",
        "\n",
        "    # Method 3: Last significant number in text\n",
        "    numbers = re.findall(r'\\\\b(\\\\d+(?:,\\\\d{3})*(?:\\\\.\\\\d+)?)\\\\b', text)\n",
        "    if numbers:\n",
        "        significant_numbers = [n for n in numbers if float(n.replace(',', '')) >= 1]\n",
        "        if significant_numbers:\n",
        "            return significant_numbers[-1]\n",
        "\n",
        "    return None\n",
        "\n",
        "def clean_answer(answer_str: str) -> str:\n",
        "    \"\"\"Clean and normalize answer\"\"\"\n",
        "    if not answer_str:\n",
        "        return \"\"\n",
        "\n",
        "    # Remove whitespace, commas, dollar signs\n",
        "    cleaned = re.sub(r'[,$\\\\s]', '', str(answer_str).strip())\n",
        "\n",
        "    # Convert to int if whole number\n",
        "    try:\n",
        "        if '.' in cleaned:\n",
        "            float_val = float(cleaned)\n",
        "            if float_val.is_integer():\n",
        "                return str(int(float_val))\n",
        "            else:\n",
        "                return str(float_val)\n",
        "        else:\n",
        "            return str(int(cleaned))\n",
        "    except ValueError:\n",
        "        return cleaned\n",
        "'''\n",
        "\n",
        "# Save the evaluator\n",
        "rft_evaluator_filename = \"kd-rft-evaluator.py\"\n",
        "with open(rft_evaluator_filename, 'w') as f:\n",
        "    f.write(rft_evaluator_code)"
      ],
      "metadata": {
        "id": "p_OE5gVatXfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting Up the RFT Training Job\n",
        "\n",
        "**Manual Setup Required**: Due to the complexity of reinforcement learning, some setup must be done through the Fireworks dashboard."
      ],
      "metadata": {
        "id": "cTBB1NPcefg4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload Evaluator function\n",
        "\n",
        "1. Go to https://app.fireworks.ai/dashboard/evaluations\n",
        "2. Click 'Create Evaluator'\n",
        "3. Name: `kd-rft-evaluator`\n",
        "4. Upload the RFT dataset we created\n",
        "5. Copy-paste the evaluator code from kd-rft-evaluator.py\n",
        "6. Save the evaluator\n",
        "\n",
        "Then create RFT job:\n",
        "\n",
        "7. Navigate to the Fine-Tuning tab.\n",
        "8. Click \"Fine-Tune a Model\" and select Reinforcement.\n",
        "9. Configure the job:\n",
        "- Model Selection: Select the model. (the model that's already fine tuned using sft; `job.output_model` to find the name)\n",
        "- Dataset: Select the `kd-rft-dataset` you uploaded.\n",
        "- Evaluator: Select the `kd-rft-evaluator` you just created.\n",
        "- Rollout: You can leave these as the default values.\n",
        "- Optional Settings: You can leave the Model Output Name blank and get the default name, or enter a name of your choosing. Store this name; it will be required in the next cell.\n",
        "10. You can leave most other hyperparameters as their defaults.\n",
        "11. Click \"Create Job\"."
      ],
      "metadata": {
        "id": "pIw4jaJsfGYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deploying the Fine-Tuned Model"
      ],
      "metadata": {
        "id": "Q5A_NwJMLUnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rft_llm = LLM(\n",
        "    model=<rft-model-output-name>,\n",
        "    deployment_type=\"on-demand\",\n",
        "    id=\"kd-rft-model\",\n",
        "    min_replica_count=0,\n",
        "    max_replica_count=1\n",
        ")\n",
        "rft_llm.apply()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "U1jDgTgxLYo6",
        "outputId": "3305ee1b-823b-49b3-fe0c-1f418f9e1d75"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLM(model=accounts/pyroworks/models/gsm8k-qwen2p5-7b-rft-0728-1-e6, deployment_url=https://app.fireworks.ai/dashboard/deployments/kd-rft-model, deployment_type=on-demand, min_replica_count=0, max_replica_count=1)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 7: Evaluate Complete Knowledge Distillation Pipeline"
      ],
      "metadata": {
        "id": "hs2-7_ACtiXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've completed our two-stage knowledge distillation pipeline (SFT for format learning, RFT for accuracy improvement), it's time to evaluate our results. But first, we need robust evaluation tools that can handle the complexity of comparing different models fairly."
      ],
      "metadata": {
        "id": "bPskuQt1O3jP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why We Need Sophisticated Evaluation Tools**\n",
        "\n",
        "The Challenge: We now have models that may respond in different formats:\n",
        "\n",
        "- Baseline model: Natural language, inconsistent formatting\n",
        "- RFT model: Structured [WORK]/[RESULT] format"
      ],
      "metadata": {
        "id": "-bWT7UqHO9lj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Problem**: Simple string matching won't work because:\n",
        "\n",
        "\n",
        "```\n",
        "# These are all the same answer but look different:\n",
        "response_1 = \"The answer is 42 dollars\"\n",
        "response_2 = \"[RESULT]\\n42\\n[/RESULT]\"  \n",
        "response_3 = \"Therefore, the total is $42.00\"\n",
        "response_4 = \"\\\\boxed{42}\"\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "o0GJ9KhiPp_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need evaluation tools that can:\n",
        "\n",
        "- Extract answers from any response format\n",
        "- Normalize numbers (handle commas, decimals, currency)\n",
        "- Track multiple metrics (accuracy, extraction success, timing)"
      ],
      "metadata": {
        "id": "o6Yd7wTAP44D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building Our Evaluation System**\n",
        "\n",
        "Let's build two essential functions that will power our model comparisons:"
      ],
      "metadata": {
        "id": "ve2DLTVrPyxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer Extraction Engine**"
      ],
      "metadata": {
        "id": "gWykgexXEyT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_answer(text: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Answer extraction that tries multiple methods\n",
        "    \"\"\"\n",
        "    # Method 0: [RESULT] tags (primary method for our SFT model)\n",
        "    result_match = re.search(r'\\[RESULT\\](.*?)\\[/RESULT\\]', text, re.DOTALL)\n",
        "    if result_match:\n",
        "        answer = result_match.group(1).strip()\n",
        "        number = extract_number_from_text(answer)\n",
        "        if number:\n",
        "            return number\n",
        "\n",
        "    # Method 1: <answer> tags\n",
        "    answer_tag_match = re.search(r'<answer>\\s*(.*?)\\s*</answer>', text, re.IGNORECASE | re.DOTALL)\n",
        "    if answer_tag_match:\n",
        "        answer = answer_tag_match.group(1).strip()\n",
        "        number = extract_number_from_text(answer)\n",
        "        if number:\n",
        "            return number\n",
        "\n",
        "    # Method 2: \\\\boxed{} format\n",
        "    boxed_match = re.search(r'\\\\boxed\\{([^}]+)\\}', text)\n",
        "    if boxed_match:\n",
        "        number = extract_number_from_text(boxed_match.group(1))\n",
        "        if number:\n",
        "            return number\n",
        "\n",
        "    # Method 3: Last number in the entire text\n",
        "    all_numbers = re.findall(r'\\b(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\b', text)\n",
        "    if all_numbers:\n",
        "        # Filter out small numbers that might be step numbers\n",
        "        significant_numbers = [n for n in all_numbers if float(n.replace(',', '')) >= 1]\n",
        "        if significant_numbers:\n",
        "            return clean_number(significant_numbers[-1])\n",
        "\n",
        "    # Method 4: \"Therefore\" or conclusion patterns\n",
        "    conclusion_patterns = [\n",
        "        r'[Tt]herefore,?\\s+.*?(\\d+(?:,\\d{3})*(?:\\.\\d+)?)',\n",
        "        r'[Ss]o,?\\s+.*?(\\d+(?:,\\d{3})*(?:\\.\\d+)?)',\n",
        "        r'[Tt]hus,?\\s+.*?(\\d+(?:,\\d{3})*(?:\\.\\d+)?)',\n",
        "        r'[Ii]n total,?\\s+.*?(\\d+(?:,\\d{3})*(?:\\.\\d+)?)',\n",
        "        r'[Aa]ltogether,?\\s+.*?(\\d+(?:,\\d{3})*(?:\\.\\d+)?)',\n",
        "    ]\n",
        "\n",
        "    for pattern in conclusion_patterns:\n",
        "        matches = re.findall(pattern, text)\n",
        "        if matches:\n",
        "            return clean_number(matches[-1])  # Take the last match\n",
        "\n",
        "    # Method 5: \"The answer is\" patterns\n",
        "    answer_is_patterns = [\n",
        "        r'[Tt]he answer is\\s+(\\d+(?:,\\d{3})*(?:\\.\\d+)?)',\n",
        "        r'[Aa]nswer:\\s*(\\d+(?:,\\d{3})*(?:\\.\\d+)?)',\n",
        "        r'[Ff]inal answer:\\s*(\\d+(?:,\\d{3})*(?:\\.\\d+)?)',\n",
        "    ]\n",
        "\n",
        "    for pattern in answer_is_patterns:\n",
        "        match = re.search(pattern, text)\n",
        "        if match:\n",
        "            return clean_number(match.group(1))\n",
        "\n",
        "    # Method 6: Numbers at the end of sentences\n",
        "    sentences = text.split('.')\n",
        "    for sentence in reversed(sentences[-3:]):  # Check last 3 sentences\n",
        "        numbers = re.findall(r'\\b(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\b', sentence)\n",
        "        if numbers:\n",
        "            return clean_number(numbers[-1])\n",
        "\n",
        "    return None\n",
        "\n",
        "def extract_number_from_text(text: str) -> Optional[str]:\n",
        "    \"\"\"Extract the main number from a piece of text\"\"\"\n",
        "    # Look for numbers, prioritizing larger ones\n",
        "    numbers = re.findall(r'\\b(\\d+(?:,\\d{3})*(?:\\.\\d+)?)\\b', text)\n",
        "    if numbers:\n",
        "        return clean_number(numbers[-1])  # Take the last/most significant number\n",
        "    return None\n",
        "\n",
        "def clean_number(number_str: str) -> str:\n",
        "    \"\"\"Clean and normalize number string\"\"\"\n",
        "    # Remove commas and extra whitespace\n",
        "    cleaned = number_str.replace(',', '').strip()\n",
        "\n",
        "    # Convert to int if it's a whole number\n",
        "    try:\n",
        "        if '.' in cleaned:\n",
        "            float_val = float(cleaned)\n",
        "            if float_val.is_integer():\n",
        "                return str(int(float_val))\n",
        "            else:\n",
        "                return str(float_val)\n",
        "        else:\n",
        "            return str(int(cleaned))\n",
        "    except ValueError:\n",
        "        return cleaned\n"
      ],
      "metadata": {
        "id": "n1Mgy-fLErbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation System**"
      ],
      "metadata": {
        "id": "cesek5UqQB7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(MODEL, deployment_id, problems):\n",
        "    \"\"\"Evaluate model\"\"\"\n",
        "\n",
        "    results = []\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    extraction_failures = 0\n",
        "\n",
        "    for i in range(0, len(problems)):\n",
        "      problem = problems[i]\n",
        "\n",
        "      # Get model response\n",
        "      llm = LLM(\n",
        "        model=MODEL,\n",
        "        deployment_type=\"on-demand\",\n",
        "        id=deployment_id  # The deployment ID that already exists\n",
        "      )\n",
        "\n",
        "      response = llm.chat.completions.create(\n",
        "          messages=[{\"role\": \"user\", \"content\": problem[\"question\"]}]\n",
        "      )\n",
        "      model_response = response.choices[0].message.content\n",
        "      model_answer = extract_answer(model_response)  # Use answer extraction\n",
        "      ground_truth = problem[\"ground_truth\"]\n",
        "\n",
        "      # Track extraction failures\n",
        "      if model_answer is None:\n",
        "          extraction_failures += 1\n",
        "\n",
        "      # Check correctness (only if we extracted something)\n",
        "      is_correct = model_answer == ground_truth if model_answer else False\n",
        "      if is_correct:\n",
        "          correct += 1\n",
        "      total += 1\n",
        "\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "EsZ3qxTCMLWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Base model Performance"
      ],
      "metadata": {
        "id": "MkwS9xTjj7nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_accuracy = evaluate_model(\n",
        "    \"qwen2p5-7b\",\n",
        "    \"kd-base-model\",\n",
        "    gsm8k_test_problems\n",
        ")\n",
        "\n",
        "rft_accuracy = evaluate_model(\n",
        "    rft_model_name,\n",
        "    \"kd_rft_model\",\n",
        "    gsm8k_test_problems\n",
        ")"
      ],
      "metadata": {
        "id": "40cgWL6qkAUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actual Results Analysis\n",
        "\n",
        "```\n",
        "ACCURACY PROGRESSION:\n",
        "Base Model:  52%\n",
        "→ RFT:       70% (+18pp)\n",
        "Total Gain:  +18 percentage point improvement\n",
        "\n",
        "FORMAT COMPLIANCE:\n",
        "SFT Model:  ~95% use [WORK]/[RESULT] format automatically  \n",
        "RFT Model:  ~95% maintain format + higher accuracy\n",
        "```"
      ],
      "metadata": {
        "id": "wNljwCOEtw0M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Course Summary and Key Takeaways"
      ],
      "metadata": {
        "id": "pzy2eJGzt1gl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What We Demonstrated\n",
        "\n",
        "**1. SFT for Internalized Format Learning**:\n",
        "- **Training Strategy**: Include format examples without system prompts in training data\n",
        "- **Testing Strategy**: No system prompts needed - format is internalized  \n",
        "- **Result**: Model automatically uses `[WORK]/[RESULT]` structure as default behavior\n",
        "- **Key Insight**: SFT teaches \"how to respond\" by making patterns the model's natural behavior\n",
        "\n",
        "**2. RFT for Accuracy Improvement**:\n",
        "- **Foundation**: Builds on SFT model\n",
        "- **Optimization**: Reward-based learning improves content quality within learned structure\n",
        "- **Result**: Maintains format compliance while significantly improving reasoning accuracy\n",
        "- **Key Insight**: RFT optimizes \"what to respond with\" while preserving structural learning\n",
        "\n",
        "**3. Two-Stage Pipeline Synergy**:\n",
        "- **Stage 1 (SFT)**: Establishes reliable, consistent response structure\n",
        "- **Stage 2 (RFT)**: Optimizes content quality within that structure\n",
        "- **Combined Result**: Models that are both well-formatted AND accurate\n",
        "\n",
        "### Practical Applications\n",
        "\n",
        "This knowledge distillation approach is valuable for:\n",
        "\n",
        "- **API Integrations**: Reliable output parsing + improved accuracy\n",
        "- **Structured Reasoning Tasks**: Clear thinking process + better results  \n",
        "- **Production Pipelines**: Consistent format + higher quality content\n",
        "- **Evaluation Systems**: Easy answer extraction + improved performance\n",
        "- **Cost Optimization**: Small models with large model capabilities\n",
        "\n",
        "### Expected Timeline and Resources\n",
        "\n",
        "- **Data Generation**: ~30 minutes (1000 examples)\n",
        "- **SFT Training**: ~45 minutes (format learning)\n",
        "- **RFT Training**: ~90 minutes (accuracy optimization)  \n",
        "- **Total Pipeline**: ~3 hours for complete format + accuracy improvement\n",
        "- **Cost**: ~TBD in compute for complete pipeline"
      ],
      "metadata": {
        "id": "SsjxPzN3t4Aw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "rfDnUHgPt6M3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tutorial demonstrated how to systematically apply knowledge distillation using Fireworks AI's platform to create models that combine the structural reliability of supervised learning with the performance optimization of reinforcement learning.\n",
        "\n",
        "**Key Success Factors**:\n",
        "1. **Clear separation of concerns**: SFT for structure, RFT for accuracy\n",
        "2. **Consistent evaluation methodology**: Test without system prompts to measure true learning\n",
        "3. **Building on foundations**: RFT builds on SFT rather than starting from scratch\n",
        "4. **Quality training data**: High teacher model accuracy and format consistency\n",
        "\n",
        "The result is a compact, efficient model that maintains the reasoning capabilities and output structure of much larger models, making it suitable for production deployment at significantly lower cost and latency.\n",
        "\n",
        "**Next Steps**: Apply this methodology to your own domain-specific tasks by:\n",
        "1. Defining appropriate output formats for your use case\n",
        "2. Generating high-quality teacher demonstrations\n",
        "3. Following the tuning pipeline\n",
        "4. Evaluating both structural and performance improvements\n",
        "\n",
        "This systematic approach to knowledge distillation enables you to create specialized, efficient models that retain the capabilities of their larger teachers while being practical for real-world deployment."
      ],
      "metadata": {
        "id": "mPujNUO9t6fT"
      }
    }
  ]
}